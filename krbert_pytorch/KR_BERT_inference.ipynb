{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oudgx27cfXtp"
      },
      "outputs": [],
      "source": [
        "## import packages\n",
        "\n",
        "import torch\n",
        "from transformers import BertConfig, BertModel, BertForPreTraining, BertTokenizer\n",
        "from unicodedata import normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co_G4o8yeyW_",
        "outputId": "86358ce0-2c2a-44bd-ffde-1ea40df08b6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1681: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "## tokenizing preprocessing\n",
        "\n",
        "tokenizer_subchar = BertTokenizer.from_pretrained('pretrained/vocab_snu_subchar12367.txt', do_lower_case=False)\n",
        "tokenizer_char = BertTokenizer.from_pretrained('pretrained/vocab_snu_char16424.txt', do_lower_case=False)\n",
        "\n",
        "# convert a string into sub-char\n",
        "def to_subchar(string):\n",
        "    return normalize('NFKD', string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wya5wf5VgmtH",
        "outputId": "4fb16c54-f802-41e0-b4bf-58b2d1717455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "subchar:  ['데이터', '처리', '##를', '위한', '문자', '##열', '예', '##시', '##입니다', '.', '배', '##ᆺ', '##사람', '.', '추', '##ᆸ다', '.']\n",
            "   char:  ['데이터', '처리', '##를', '위한', '문자', '##열', '예', '##시', '##입니다', '.', '뱃', '##사람', '.', '춥', '##다', '.']\n"
          ]
        }
      ],
      "source": [
        "sentence = '데이터 처리를 위한 문자열 예시입니다. 뱃사람. 춥다.'\n",
        "\n",
        "print(\"subchar: \", tokenizer_subchar.tokenize(to_subchar(sentence)))\n",
        "print(\"   char: \", tokenizer_char.tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvEHOWblgm7i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxBpnJMkgm_H"
      },
      "outputs": [],
      "source": [
        "## Downstream tasks (Naver Sentiment Movie Corpus)\n",
        "## Based on 'KR-BERT character Bidirectional WordPiece'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B21SoDvFrJ18"
      },
      "source": [
        "Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uOaJt8EvSg8l"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import pickle\n",
        "import json\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from pretrained.tokenization_ranked import FullTokenizer as KBertRankedTokenizer\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from model.net import SentenceClassifier\n",
        "from model.data import Corpus\n",
        "from model.utils import PreProcessor, PadSequence\n",
        "from model.metric import evaluate, acc\n",
        "from utils import Config, CheckpointManager, SummaryManager\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I0eWvlhmSjpR"
      },
      "outputs": [],
      "source": [
        "# set path\n",
        "\n",
        "ptr_dir = Path('pretrained')\n",
        "data_dir = Path('data')\n",
        "model_dir = Path('checkpoints')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eRwCELnWSjss"
      },
      "outputs": [],
      "source": [
        "# load configs and vocab\n",
        "\n",
        "ptr_config = Config(ptr_dir / 'config_char16424_ranked.json')\n",
        "data_config = Config(data_dir / 'config.json')\n",
        "model_config = Config('finetuning_config.json')\n",
        "with open(ptr_config.config, mode=\"r\") as io :\n",
        "    bert_config = json.loads(io.read())\n",
        "\n",
        "vocab = pickle.load(open(ptr_config.vocab, mode='rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oJMVhhhUqUA",
        "outputId": "09c79012-e84c-432b-9c06-c0167831c3f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CONVERT A STRING INTO CHAR]\n"
          ]
        }
      ],
      "source": [
        "# load preprocessor\n",
        "\n",
        "ptr_tokenizer = KBertRankedTokenizer(ptr_config.tokenizer, do_lower_case=False)\n",
        "pad_sequence = PadSequence(length=model_config.length, pad_val=vocab.to_indices(vocab.padding_token))\n",
        "preprocessor = PreProcessor(vocab=vocab, split_fn=ptr_tokenizer.tokenize, pad_fn=pad_sequence, subchar='False')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = BertConfig(**bert_config)\n",
        "model = SentenceClassifier(config, num_classes=model_config.num_classes, vocab=preprocessor.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9uo74uoUqq-",
        "outputId": "5e0fb801-f77c-4a99-b7ef-a05788ef7419"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_pretrained = torch.load('checkpoints/best_snu_char16424_ranked.tar', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(bert_pretrained['model_state_dict'], strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RrvxhdlUqtV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EjZDSTVvlXn",
        "outputId": "13913f7d-4b5b-4b5b-a7a5-cbdc2a567a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_tokens:\n",
            "tensor([[    2,   693,  1598,  1499,   873,  4239,   192,   790,     5,     5,\n",
            "          2964,  6386,   419,     3,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    2, 13588,  1592,   708,   142,   367,     7,   254,    81,  1448,\n",
            "           745,  2117,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    2,  4966,   180,  2858,    32,   743, 13840,   978,  2712,   254,\n",
            "             5,     5,  1185,   449, 12167,     9,   254,   161,    23,   551,\n",
            "          1158,    13,   488,  2006,   466,     3,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1],\n",
            "        [    2,   795,   254,  1401,    66,    79,     3,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1]])\n"
          ]
        }
      ],
      "source": [
        "txt_1 = '이런걸 왜 돈 주고 보냐.. 진짜 최악임'\n",
        "token_1 = torch.tensor(preprocessor.preprocess(txt_1)).view(1, -1)\n",
        "\n",
        "txt_2 = '짱 좋아 최고야 올해의 영화상 드립니다'\n",
        "token_2 = torch.tensor(preprocessor.preprocess(txt_2)).view(1, -1)\n",
        "\n",
        "txt_3 = '그냥 나쁘지 않은 킬링타임 영화.. 근데 굳이 영화관에서 볼 필요는 없는듯?'\n",
        "token_3 = torch.tensor(preprocessor.preprocess(txt_3)).view(1, -1)\n",
        "\n",
        "txt_4 = '감독은 영화 접어라'\n",
        "token_4 = torch.tensor(preprocessor.preprocess(txt_4)).view(1, -1)\n",
        "\n",
        "tokens = torch.cat((token_1, token_2, token_3, token_4), 0)\n",
        "print('test_tokens:')\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bNPYwEbvlaA",
        "outputId": "ddd1a11d-af91-40fa-9dfd-ffadadcbeeb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['NEGATIVE', 'POSITIVE', 'POSITIVE', 'NEGATIVE']\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "output = model(tokens)\n",
        "print(['NEGATIVE' if o < 0.5 else 'POSITIVE' for o in output.max(dim=1)[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HtMN7F8-jvSB"
      },
      "outputs": [],
      "source": [
        "attention_mask = tokens.ne(model.vocab.to_indices(model.vocab.padding_token)).float()\n",
        "_, pooled_output = model.bert(input_ids=tokens, attention_mask=attention_mask, return_dict=False)\n",
        "pooled_output = model.dropout(pooled_output)\n",
        "logits = model.classifier(pooled_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2zoENbaA3lv",
        "outputId": "1ef93c25-e2a1-4de6-d5c3-4f9fa3572247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 2.8851, -2.8254],\n",
            "        [-2.2497,  2.4242],\n",
            "        [-1.7892,  1.9156],\n",
            "        [ 1.3974, -1.4130]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(logits)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "KR-BERT-inference.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
